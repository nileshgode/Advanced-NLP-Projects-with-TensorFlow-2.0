{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sarcasm_detection using CNN",
      "provenance": [],
      "authorship_tag": "ABX9TyM0Qb3OXXSj/YDrCjsMyHU8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nileshgode/Advanced-NLP-Projects-with-TensorFlow-2.0/blob/master/Sarcasm_detection_using_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XtuUBboroC1",
        "colab_type": "text"
      },
      "source": [
        "## Sarcasm Detection using Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN67tUfn39Vs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "40b5a49f-9316-42e2-9d91-d345d379e825"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5oDdUljsB31",
        "colab_type": "text"
      },
      "source": [
        "Import the json Data file in drive for read by pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NawikRdR5a4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks\")\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZxkLOowk3H_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_json('/content/drive/My Drive/Colab Notebooks/Sarcasm_Headlines_Dataset_v2.json', lines=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwN5YDcxlE0w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "d255175c-269b-43ec-dad0-983f6d8abe25"
      },
      "source": [
        "data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline</th>\n",
              "      <th>article_link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
              "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>dem rep. totally nails why congress is falling...</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>inclement weather prevents liar from getting t...</td>\n",
              "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>mother comes pretty close to using word 'strea...</td>\n",
              "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28614</th>\n",
              "      <td>1</td>\n",
              "      <td>jews to celebrate rosh hashasha or something</td>\n",
              "      <td>https://www.theonion.com/jews-to-celebrate-ros...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28615</th>\n",
              "      <td>1</td>\n",
              "      <td>internal affairs investigator disappointed con...</td>\n",
              "      <td>https://local.theonion.com/internal-affairs-in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28616</th>\n",
              "      <td>0</td>\n",
              "      <td>the most beautiful acceptance speech this week...</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/andrew-ah...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28617</th>\n",
              "      <td>1</td>\n",
              "      <td>mars probe destroyed by orbiting spielberg-gat...</td>\n",
              "      <td>https://www.theonion.com/mars-probe-destroyed-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28618</th>\n",
              "      <td>1</td>\n",
              "      <td>dad clarifies this not a food stop</td>\n",
              "      <td>https://www.theonion.com/dad-clarifies-this-no...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>28619 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       is_sarcastic  ...                                       article_link\n",
              "0                 1  ...  https://www.theonion.com/thirtysomething-scien...\n",
              "1                 0  ...  https://www.huffingtonpost.com/entry/donna-edw...\n",
              "2                 0  ...  https://www.huffingtonpost.com/entry/eat-your-...\n",
              "3                 1  ...  https://local.theonion.com/inclement-weather-p...\n",
              "4                 1  ...  https://www.theonion.com/mother-comes-pretty-c...\n",
              "...             ...  ...                                                ...\n",
              "28614             1  ...  https://www.theonion.com/jews-to-celebrate-ros...\n",
              "28615             1  ...  https://local.theonion.com/internal-affairs-in...\n",
              "28616             0  ...  https://www.huffingtonpost.com/entry/andrew-ah...\n",
              "28617             1  ...  https://www.theonion.com/mars-probe-destroyed-...\n",
              "28618             1  ...  https://www.theonion.com/dad-clarifies-this-no...\n",
              "\n",
              "[28619 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pItJc_47sV85",
        "colab_type": "text"
      },
      "source": [
        "Import other Necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuqdnWRvlRLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "% matplotlib inline\n",
        "import numpy as np\n",
        "import re\n",
        "import gensim\n",
        "import math\n",
        "import nltk\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# supress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2y1XjXglY0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_data(file):\n",
        "    for l in open(file,'r'):\n",
        "        yield json.loads(l)\n",
        "\n",
        "data = list(parse_data('Sarcasm_Headlines_Dataset_v2.json'))\n",
        "df = pd.DataFrame(data)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_yxDc7Iln-h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "cc8c0761-d5dc-4951-ceb2-97dc0cac77e4"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline</th>\n",
              "      <th>article_link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
              "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>dem rep. totally nails why congress is falling...</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>inclement weather prevents liar from getting t...</td>\n",
              "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>mother comes pretty close to using word 'strea...</td>\n",
              "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   is_sarcastic  ...                                       article_link\n",
              "0             1  ...  https://www.theonion.com/thirtysomething-scien...\n",
              "1             0  ...  https://www.huffingtonpost.com/entry/donna-edw...\n",
              "2             0  ...  https://www.huffingtonpost.com/entry/eat-your-...\n",
              "3             1  ...  https://local.theonion.com/inclement-weather-p...\n",
              "4             1  ...  https://www.theonion.com/mother-comes-pretty-c...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mauSsFhstPh9",
        "colab_type": "text"
      },
      "source": [
        "As Above Data Contain Binary Values given each headline, article_link is not useful for us so we will remove that with .pop command"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQzfF9tts_Hp",
        "colab_type": "text"
      },
      "source": [
        "**Performing basic data analysis and preprocessing our data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWFoLHKXluil",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "a0888c7f-1c76-43db-c6a2-ef7f20df00f0"
      },
      "source": [
        "df.pop('article_link')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        https://www.theonion.com/thirtysomething-scien...\n",
              "1        https://www.huffingtonpost.com/entry/donna-edw...\n",
              "2        https://www.huffingtonpost.com/entry/eat-your-...\n",
              "3        https://local.theonion.com/inclement-weather-p...\n",
              "4        https://www.theonion.com/mother-comes-pretty-c...\n",
              "                               ...                        \n",
              "28614    https://www.theonion.com/jews-to-celebrate-ros...\n",
              "28615    https://local.theonion.com/internal-affairs-in...\n",
              "28616    https://www.huffingtonpost.com/entry/andrew-ah...\n",
              "28617    https://www.theonion.com/mars-probe-destroyed-...\n",
              "28618    https://www.theonion.com/dad-clarifies-this-no...\n",
              "Name: article_link, Length: 28619, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbF8KV5rlxc6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "59b54bf6-83cc-4199-cd6c-9400e05d50ec"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28619"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSnTsbWCl0T3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8c91a544-22d3-4ca1-9568-a921a6c8f72c"
      },
      "source": [
        "df['headline'][4]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"mother comes pretty close to using word 'streaming' correctly\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5wAzW4fl28y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ea328cb4-3ad8-412b-bca6-b96440a43695"
      },
      "source": [
        "df['headline'][1]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'dem rep. totally nails why congress is falling short on gender, racial equality'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3krpnDal5lW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d9398bd0-3e1b-4ef8-e1b7-fa65b47d6109"
      },
      "source": [
        "classes = np.unique(np.array(df['is_sarcastic']))\n",
        "classes"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8WIu7lUuTgn",
        "colab_type": "text"
      },
      "source": [
        "Performing basic data analysis and preprocessing our data by performing following steps:\n",
        "1. Remove special characters.\n",
        "2. Keep only alphanumeric data.\n",
        "3. Remove stopwords\n",
        "4. Lemmatize our data\n",
        "5. Perform case-folding on the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oJK2cM2l84n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f59962e7-f9e9-4f8f-9aa4-d1d0ffa59d8f"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def text_clean(corpus):\n",
        "    '''\n",
        "    Purpose : Function to keep only alphabets, digits and certain words (punctuations, qmarks, tabs etc. removed)\n",
        "    \n",
        "    Input : Takes a text corpus, 'corpus' to be cleaned along with a list of words, 'keep_list', which have to be retained\n",
        "            even after the cleaning process\n",
        "    \n",
        "    Output : Returns the cleaned text corpus\n",
        "    \n",
        "    '''\n",
        "    cleaned_corpus = pd.Series()\n",
        "    for row in corpus:\n",
        "        qs = []                                                                # Collect Clean text as a series\n",
        "        for word in row.split():\n",
        "            p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)           # With Regx remove any numbers in words\n",
        "            p1 = p1.lower()                                                    # Convert into Lower Case\n",
        "            qs.append(p1)                                                      # Collected text append into 'qs'\n",
        "        cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
        "    return cleaned_corpus"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91UNxM_6l__v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Removal of Stopwords if Any \n",
        "def stopwords_removal(corpus):\n",
        "    stop = set(stopwords.words('english'))\n",
        "    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
        "    return corpus"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v5eL7CImC4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Convert original text into its lematize form \n",
        "def lemmatize(corpus):\n",
        "    lem = WordNetLemmatizer()\n",
        "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
        "    return corpus\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF3FFXvWmG91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Perform Stemming Operation on corpus, Snowball has faster operation than porter stemmer\n",
        "def stem(corpus, stem_type = None):\n",
        "    if stem_type == 'snowball':\n",
        "        stemmer = SnowballStemmer(language = 'english')\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    else :\n",
        "        stemmer = PorterStemmer()\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    return corpus"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL7K99yWmJ36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(corpus, cleaning = True, stemming = False, stem_type = None, lemmatization = False, remove_stopwords = True):\n",
        "    \n",
        "    '''\n",
        "    Purpose : Function to perform all pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal etc.)\n",
        "    \n",
        "    Input : \n",
        "    'corpus' - Text corpus on which pre-processing tasks will be performed\n",
        "    \n",
        "    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should \n",
        "                                                                  be performed or not\n",
        "    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter\n",
        "                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n",
        "    \n",
        "    Note : Either stemming or lemmatization should be used. There's no benefit of using both of them together\n",
        "    \n",
        "    Output : Returns the processed text corpus\n",
        "    \n",
        "    '''\n",
        "    if cleaning == True:\n",
        "        corpus = text_clean(corpus)\n",
        "    \n",
        "    if remove_stopwords == True:\n",
        "        corpus = stopwords_removal(corpus)\n",
        "    else :\n",
        "        corpus = [[x for x in x.split()] for x in corpus]\n",
        "    \n",
        "    if lemmatization == True:\n",
        "        corpus = lemmatize(corpus)\n",
        "        \n",
        "        \n",
        "    if stemming == True:\n",
        "        corpus = stem(corpus, stem_type)\n",
        "    \n",
        "    corpus = [' '.join(x) for x in corpus]\n",
        "        \n",
        "\n",
        "    return corpus"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y-UxTl2mP8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headlines = preprocess(df['headline'], lemmatization = True, remove_stopwords = True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5Yi-p-imZtv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b34c1859-e2df-4b9e-c62a-4f6ad89b59dc"
      },
      "source": [
        "headlines[0:5]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['thirtysomething scientists unveil doomsday clock hair loss',\n",
              " 'dem rep totally nail congress fall short gender racial equality',\n",
              " 'eat veggies 9 deliciously different recipes',\n",
              " 'inclement weather prevent liar get work',\n",
              " 'mother come pretty close use word stream correctly']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srWpmR7kmcmt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4adc78b2-c666-4e9c-eb09-5a37aedfeb52"
      },
      "source": [
        "## Import the word2vec Model, check for its availability first, if not present dowload it from api and load 'word2vec-google-news-300'\n",
        "## It will take about 10 minute for first download to load near about 1.5GB data\n",
        "## Once It will be loaded, we can do further process\n",
        "\n",
        "from gensim.models import Word2Vec \n",
        "import gensim.downloader as api\n",
        "    \n",
        "v2w_model=None;\n",
        "try:\n",
        "    v2w_model = gensim.models.KeyedVectors.load(\"./w2vecmodel.mod\")\n",
        "    print(\"Loaded w2v model\")\n",
        "except:            \n",
        "    v2w_model = api.load('word2vec-google-news-300')\n",
        "    v2w_model.save(\"./w2vecmodel.mod\")\n",
        "    print(\"Saved w2vec model\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded w2v model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Iy-tby_n05_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## send the word vectors, each of size 300, to our CNN model.\n",
        "## If any of the headlines contain more than 10 characters, they will be subsampled to keep the word embeddings for the first 10 characters. \n",
        "## If any of the headlines contain less than 10 characters, we will pad them so that they have vectors with values of 0.\n",
        "\n",
        "MAX_LENGTH = 10\n",
        "VECTOR_SIZE = 300"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IXJL1rhn3lU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 1. convert the preprocessed headlines into their respective vectors\n",
        "## 2. pad or subsample the data based on whether the size of the data is greater or less than the maximum length we have defined\n",
        "\n",
        "def vectorize_data(data):\n",
        "    \n",
        "    vectors = []\n",
        "    \n",
        "    padding_vector = [0.0] * VECTOR_SIZE\n",
        "    \n",
        "    for i, data_point in enumerate(data):\n",
        "        data_point_vectors = []\n",
        "        count = 0\n",
        "        \n",
        "        tokens = data_point.split()\n",
        "        \n",
        "        for token in tokens:\n",
        "            if count >= MAX_LENGTH:\n",
        "                break\n",
        "            if token in v2w_model.wv.vocab:\n",
        "                data_point_vectors.append(v2w_model.wv[token])\n",
        "            count = count + 1\n",
        "        \n",
        "        if len(data_point_vectors) < MAX_LENGTH:\n",
        "            to_fill = MAX_LENGTH - len(data_point_vectors)\n",
        "            for _ in range(to_fill):\n",
        "                data_point_vectors.append(padding_vector)\n",
        "        \n",
        "        vectors.append(data_point_vectors)\n",
        "        \n",
        "    return vectors"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uyaIyNSn60z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## The 'vectorized_headlines' parameter contains our headlines after they've been converted into vectors using Word2Vec. \n",
        "\n",
        "vectorized_headlines = vectorize_data(headlines)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoBA7ThSn9mm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Add a small validation to ensure that the 10 vectors are present for each headline\n",
        "for i, vec in enumerate(vectorized_headlines):\n",
        "    if len(vec) != MAX_LENGTH:\n",
        "        print(i)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ3tDxSloI-f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad411032-a2ca-4788-e6c5-69b95bb0f5f5"
      },
      "source": [
        "len(vectorized_headlines[1])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdvdsOpGoLyM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b907a343-c02b-47c4-abf0-09c3998bb3c9"
      },
      "source": [
        "len(vectorized_headlines)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28619"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHQy9QfooPAX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97867c83-7e2e-4fd8-cdbb-5daa8c29fa41"
      },
      "source": [
        "## Select Data train:test ratio as 70:30\n",
        "train_div = math.floor(0.7 * len(vectorized_headlines))\n",
        "train_div"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20033"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMv7eYeDoR76",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "96c88032-fff1-4f0c-9369-bc597e8cd692"
      },
      "source": [
        "## split our data into train and test sets\n",
        "\n",
        "X_train = vectorized_headlines[:train_div]\n",
        "y_train = df['is_sarcastic'][:train_div]\n",
        "X_test = vectorized_headlines[train_div:]\n",
        "y_test = df['is_sarcastic'][train_div:]\n",
        "\n",
        "print('The size of X_train is:', len(X_train), '\\nThe size of y_train is:', len(y_train),\n",
        "      '\\nThe size of X_test is:', len(X_test), '\\nThe size of y_test is:', len(y_test))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size of X_train is: 20033 \n",
            "The size of y_train is: 20033 \n",
            "The size of X_test is: 8586 \n",
            "The size of y_test is: 8586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H135JqlooWlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Need to reshape our data in order to convert it into the form expected by our CNN model\n",
        "\n",
        "X_train = np.reshape(X_train, (len(X_train), MAX_LENGTH, VECTOR_SIZE))\n",
        "X_test = np.reshape(X_test, (len(X_test), MAX_LENGTH, VECTOR_SIZE))\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNktHJ5x2nFg",
        "colab_type": "text"
      },
      "source": [
        "## Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dstk8glY2rLr",
        "colab_type": "text"
      },
      "source": [
        "we are ready with our vectorized data, let's get into the CNN part. We begin by defining the hyperparameters of our network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7BzB5HTocxg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64f69b4c-9def-49cb-91a3-3bd39183b0ea"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras import layers\n",
        "from keras.layers import Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "### Select the define hyperparameters\n",
        "\n",
        "FILTERS= 10                                    ## The number of filters.\n",
        "KERNEL_SIZE=3                                  ## indicates the number of tokens in the text\n",
        "HIDDEN_LAYER_1_NODES= 20                       ## The number of nodes to be used in each of the hidden layers\n",
        "HIDDEN_LAYER_2_NODES= 10            \n",
        "DROPOUT_PROB=0.35                              ## indicating the percentage of nodes to be dropped at random.\n",
        "NUM_EPOCHS=15                                  ## The number of epochs we'll see the entire data.\n",
        "BATCH_SIZE=50                                  ## The number of vectorized headlines to input to the model in each batch."
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExXeGDvwofom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(FILTERS,\n",
        "                 KERNEL_SIZE,\n",
        "                 padding='same',\n",
        "                 strides=1,                                       ## defined a stride of 1\n",
        "                 activation='relu',                               ## ReLU as the activation function\n",
        "                 input_shape = (MAX_LENGTH, VECTOR_SIZE)))\n",
        "model.add(GlobalMaxPooling1D())                             ## used one-dimensional convolutions due to the single dimensionality associated with text data\n",
        "model.add(Dense(HIDDEN_LAYER_1_NODES, activation='relu'))   ## define our feedforward neural network, along with the dropout layers\n",
        "model.add(Dropout(DROPOUT_PROB))\n",
        "model.add(Dense(HIDDEN_LAYER_2_NODES, activation='relu'))\n",
        "model.add(Dropout(DROPOUT_PROB))\n",
        "\n",
        "## Earlier layers, we used ReLU as the activation function and sigmoid in the last layer since we are trying to solve a binary classification problem\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbvzdbaF57-v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "23c14e26-8f87-4f6b-98d2-35a06e23ae88"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, 10, 10)            9010      \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                220       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                210       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 9,451\n",
            "Trainable params: 9,451\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0T_cKCnojRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuAm5Z7xooat",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "3c1ec079-56f9-4b39-967c-6b4774b36be4"
      },
      "source": [
        "training_history = model.fit(X_train, y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "20033/20033 [==============================] - 3s 132us/step - loss: 0.6234 - accuracy: 0.6572\n",
            "Epoch 2/15\n",
            "20033/20033 [==============================] - 2s 99us/step - loss: 0.5256 - accuracy: 0.7520\n",
            "Epoch 3/15\n",
            "20033/20033 [==============================] - 2s 102us/step - loss: 0.4694 - accuracy: 0.7916\n",
            "Epoch 4/15\n",
            "20033/20033 [==============================] - 2s 101us/step - loss: 0.4342 - accuracy: 0.8135\n",
            "Epoch 5/15\n",
            "20033/20033 [==============================] - 2s 103us/step - loss: 0.4014 - accuracy: 0.8307\n",
            "Epoch 6/15\n",
            "20033/20033 [==============================] - 2s 103us/step - loss: 0.3777 - accuracy: 0.8425\n",
            "Epoch 7/15\n",
            "20033/20033 [==============================] - 2s 102us/step - loss: 0.3509 - accuracy: 0.8574\n",
            "Epoch 8/15\n",
            "20033/20033 [==============================] - 2s 103us/step - loss: 0.3268 - accuracy: 0.8675\n",
            "Epoch 9/15\n",
            "20033/20033 [==============================] - 2s 103us/step - loss: 0.3090 - accuracy: 0.8765\n",
            "Epoch 10/15\n",
            "20033/20033 [==============================] - 2s 102us/step - loss: 0.2919 - accuracy: 0.8837\n",
            "Epoch 11/15\n",
            "20033/20033 [==============================] - 2s 101us/step - loss: 0.2766 - accuracy: 0.8897\n",
            "Epoch 12/15\n",
            "20033/20033 [==============================] - 2s 102us/step - loss: 0.2625 - accuracy: 0.8955\n",
            "Epoch 13/15\n",
            "20033/20033 [==============================] - 2s 102us/step - loss: 0.2534 - accuracy: 0.9009\n",
            "Epoch 14/15\n",
            "20033/20033 [==============================] - 2s 102us/step - loss: 0.2370 - accuracy: 0.9047\n",
            "Epoch 15/15\n",
            "20033/20033 [==============================] - 2s 102us/step - loss: 0.2239 - accuracy: 0.9093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saEk555Ao2Tv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c313ba0-5d3f-42f8-9452-30a7c112af23"
      },
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Accuracy:  0.7469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4oEYipy7Ofq",
        "colab_type": "text"
      },
      "source": [
        "**We get Training Accuracy as 90.93 %  and Test Accuracy as 74.69 %**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEdIPw1-6rKX",
        "colab_type": "text"
      },
      "source": [
        "We can play with the Model Hyperparameters and observe the Training and Testing Accuracy, Focus on Testing Accuracy instead of training accuracy so that our Model should not Overfit the Data and leads to unexpected outcomes"
      ]
    }
  ]
}